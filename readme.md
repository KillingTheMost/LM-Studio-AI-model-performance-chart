hello here is the chart.

# ğŸš€ My 4090 Powerhouse Performance Chart

*(Sorted: Largest Parameters â†’ Smallest | Your Setup: RTX 4090 | 32GB DDR5 RAM | Intel i7-13900K)*

# ğŸš€ My 4090 Powerhouse Performance Chart  

*(Sorted: Largest Parameters â†’ Smallest | Your Setup: RTX 4090 | 32GB DDR5 RAM | Intel i7-13900K)*

| ğŸ§  Model Name                                | âš™ï¸ Params | ğŸ” **Quant**      | ğŸ“ Context Length | âš¡ Tokens/s   | ğŸ”‘ Fits in 24GB VRAM |
|----------------------------------------------|-----------|--------------------|--------------------|----------------|----------------------|
| `Qwen3-30B-A3B-Instruct` (Q8_0)              | **30B**   | `Q8_0`             | 32k                 | **17â€“25**     | âœ… Yes                |
| `Qwen3-Coder-30B-A3B-Instruct` (Q8_0)        | **30B**   | `Q8_0`             | 4k                  | **11**        | âš ï¸ No (spillover? Nah!) |
| `Devstral-Small` (Q6_K)                     | **24B**   | `Q6_K`             | 14k                 | **45**        | âœ… Yes                |
| `Magistral-Small` (Q6_K)                    | **24B**   | `Q6_K`             | 14k                 | **40**        | âœ… Yes                |
| `ERNIE-4.5-21B-A3B-PT` (Q6_K)               | **21B**   | `Q6_K`             | 40k                 | **150â€“160**   | âœ… Yes                |
| `Qwen3-32B-Q4_K_M` (Q4_K_M)                 | **32B**   | `Q4_K_M`           | 9k                  | **16â€“32**     | âœ… Yes                |
| `DeepSeek-R1-Distill-Llama-Medical-Doctor`  | **8B**    | `F16`              | 36k                 | **50**        | âœ… Yes                |
| `deepseek-r1-0528-qwen3-8b` (Q8_0)          | **8B**    | `Q8_0`             | 80k                 | **50â€“77**     | âœ… Yes                |
| `Qwen3-4B-Thinking` (F16)                   | **4B**    | `F16`              | 68k                 | **77**        | âœ… Yes                |

---

### ğŸ’ Why This Matters for YOU
| Your Obsession          | What You See                                                                 |
|-------------------------|----------------------------------------------------------------------------|
| **Spillovers?**         | `Qwen3-Coder-30B` uses **23.8GB VRAM** (spillovers = temporary GGML magic!)  |
| **Long context wins**   | âœ… `Qwen3-4B` hits **77 tokens/s** with **68k context** â†’ *way* faster!      |
| **Your hardware shines**| ğŸ”¥ **150â€“160 t/s** for `ERNIE-4.5-21B` (40k context) on your 4090 ğŸ‘‘       |

---

### ğŸ† Your Best Pick Right Now
```bash
# Run this to get 150â€“160 tokens/second with ZERO spillovers:
llama.cpp --model ERNIE-4.5-21B-A3B-PT-Q6_K.gguf --ctx-size 40k

---

### ğŸ† Your Best Pick Right Now
```bash
# Run this to get 150â€“160 tokens/second with ZERO spillovers:
llama.cpp --model ERNIE-4.5-21B-A3B-PT-Q6_K.gguf --ctx-size 40k
